{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import re\n",
    "import string\n",
    "import unicodedata as ud\n",
    "import codecs\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### GET DATA ##############\n",
    "\n",
    "#import  stop words \n",
    "def get_stop_words():\n",
    "    path = \"stop_words.txt\"\n",
    "    stop_words = []\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "        stop_words = myfile.readlines()\n",
    "    stop_words = [word.strip() for word in stop_words]\n",
    "    return stop_words\n",
    "\n",
    "# import Emojis \n",
    "def get_emojis():\n",
    "    with codecs.open(\"emoji.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "         positive_emoji=myfile.read()\n",
    "    positive_emoji=positive_emoji.split(\"\\r\\n\")\n",
    "    positive_emoji=positive_emoji[1:len(positive_emoji)-1] \n",
    "    with codecs.open(\"neg_emoji.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfie:\n",
    "        neg_emoji=myfie.read()\n",
    "    neg_emoji=neg_emoji.split(\"\\r\\n\")\n",
    "    neg_emoji=neg_emoji[1:len(neg_emoji)-1]\n",
    "    return positive_emoji,neg_emoji\n",
    "    \n",
    "\n",
    "#import (Text , label) \n",
    "def get_data():\n",
    "    path=\"data_text.txt\"\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "        text = myfile.read()\n",
    "    sentences=text.split(\"@\")\n",
    "    sentences=[x.split(\"\\r\\n\") for x in sentences]\n",
    "    sentences=sentences[1:len(sentences)-1]\n",
    "    df=pd.DataFrame(sentences)\n",
    "    df.columns = ['sentiment','sentence']\n",
    "    return df\n",
    "\n",
    "############## PREPROCESSING DATA ################\n",
    "\n",
    "\n",
    "#Remove stop words \n",
    "def remove_stp_words(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    stop_words = get_stop_words()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            text_words.append(word)\n",
    "    return ' '.join(text_words)\n",
    "\n",
    "\n",
    "#Emoticons_Regex \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "#Tokenizing_the_text :\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "\n",
    "#Normalize_the_text:\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Remove-repeating_char :\n",
    "def remove_repeating_char(text):\n",
    "     #return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)     # keep 2 repeat\n",
    "\n",
    "#Detecting_Emojis_REGEX\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#Transform Emojis to their sentiment \n",
    "def emoji_to_text(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    positive_emoji,neg_emoji=get_emojis()\n",
    "    for word in words:\n",
    "        if emoji_pattern.search(word):\n",
    "            if word in positive_emoji :\n",
    "                word='ايجابي'\n",
    "            if word in neg_emoji :\n",
    "                word='سلبي'\n",
    "        text_words.append(word)\n",
    "    return ' '.join(text_words)\n",
    "\n",
    "#REMOVING_Punctuation\n",
    "def remove_punc(text):\n",
    "    return ''.join(c for c in text if not ud.category(c).startswith('P'))\n",
    "\n",
    "\n",
    "#Stemmer_LIGHT : Remove suffixes and affixes \n",
    "ArListem = ArabicLightStemmer()\n",
    "def stemmer_light(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    for c in words:\n",
    "        stem = ArListem.light_stem(c)\n",
    "        text_words.append(stem)\n",
    "    return ' '.join(text_words)\n",
    "\n",
    "#Root Stemming :  Transform the wrod into its root form\n",
    "def stemmer_root(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    for c in words:\n",
    "        stem = ArListem.light_stem(c)\n",
    "        text_words.append(stem)\n",
    "    return ' '.join(text_words)\n",
    "\n",
    "\n",
    "############ DARIJA TEXT PREPROCESSIG #########\n",
    "\n",
    "def data_clean(stp_words=False,stem=True,tok=True,emojis=False):\n",
    "    data = get_data()\n",
    "    sentences = data['sentence']\n",
    "    #Remove ـــ character wich is used a lot in arabic for exhibition\n",
    "    sentences = [araby.strip_tatweel(text) for text in sentences] \n",
    "    #Remove Punctiation \n",
    "    sentences = [remove_punc(text) for text in sentences]\n",
    "    if emojis==False :\n",
    "        sentences = [emoji_to_text(text) for text in sentences] \n",
    "    #Remove Repeating character\n",
    "    sentences = [remove_repeating_char(text) for text in sentences]\n",
    "    #Remoce arabic diacritics\n",
    "    sentences = [araby.strip_tashkeel(text) for text in sentences]\n",
    "    if stp_words==True :\n",
    "        sentences = [remove_stp_words(text) for text in sentences]\n",
    "    if stem==True:\n",
    "        sentences = [stemmer_light(text) for text in sentences]\n",
    "    sentences = [normalize_arabic(text) for text in sentences]\n",
    "    sentences = [araby.normalize_hamza(text) for text in sentences]\n",
    "    if tok==True:\n",
    "        sentences = [tokenize(text) for text in sentences]\n",
    "    return sentences         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## TF_IDF ############ TERM FREQUENCY _ INVERSE DOCUMENT FREQUENCY\n",
    "def get_tfidf():\n",
    "    data,sentiment = data_clean(stp_words=True,stem=True,tok=False,emojis=False)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, sentiment, test_size=0.2, random_state=1)\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (1,1))\n",
    "    X_tr = vectorizer.fit_transform(X_train)\n",
    "    X_tes = vectorizer.transform(X_test)\n",
    "    return X_tr,X_tes,y_train,y_test\n",
    "########## Document-Term Matrix(DTM) ######\n",
    "def doc_term_frec():\n",
    "    data,sentiment = data_clean(stp_words=True,stem=True,tok=False,emojis=False)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, sentiment, test_size=0.2, random_state=1)\n",
    "    cv = CountVectorizer(ngram_range = (1,1))\n",
    "    X_tr= cv.fit_transform(X_train)\n",
    "    X_tes = cv.transform(X_test)\n",
    "    return X_tr,X_tes,y_train,y_test\n",
    "\n",
    "\n",
    "\n",
    "plt.bar([\"Positive\",\"Negative\"],[sentiment[sentiment=='P'].count(),sentiment[sentiment=='N'].count()],color=['g','b']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1642, 9112) (1642,)\n",
      "(411, 9112) (411,)\n"
     ]
    }
   ],
   "source": [
    "data_set =get_data()\n",
    "sentiment = data_set['sentiment']\n",
    "sentimen =np.array( [1 if sen == 'P' else 0 for sen in sentiment])\n",
    "\n",
    "X_train, X_test, y_train, y_test = doc_term_frec()\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82       192\n",
      "           1       0.87      0.77      0.82       219\n",
      "\n",
      "    accuracy                           0.82       411\n",
      "   macro avg       0.82      0.82      0.82       411\n",
      "weighted avg       0.82      0.82      0.82       411\n",
      "\n",
      "Logistic Regression: 0.8175182481751825 0.9987819732034104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.74      0.79       248\n",
      "           1       0.66      0.79      0.72       163\n",
      "\n",
      "    accuracy                           0.76       411\n",
      "   macro avg       0.75      0.76      0.75       411\n",
      "weighted avg       0.77      0.76      0.76       411\n",
      "\n",
      "Decision Tree Accuracy: 0.7591240875912408 0.9993909866017052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.79       182\n",
      "           1       0.88      0.74      0.80       229\n",
      "\n",
      "    accuracy                           0.80       411\n",
      "   macro avg       0.80      0.81      0.80       411\n",
      "weighted avg       0.81      0.80      0.80       411\n",
      "\n",
      "MultinomialNB Accuracy: 0.7980535279805353 0.9957369062119367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.87      0.79       181\n",
      "           1       0.88      0.74      0.80       230\n",
      "\n",
      "    accuracy                           0.80       411\n",
      "   macro avg       0.80      0.80      0.80       411\n",
      "weighted avg       0.81      0.80      0.80       411\n",
      "\n",
      "SVM Accuracy: 0.7956204379562044 0.9987819732034104\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.1, fit_prior= False).fit(X_train, y_train)\n",
    "log = LogisticRegression( solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)                                                                    \n",
    "d_tree = tree.DecisionTreeClassifier().fit(X_train,y_train)\n",
    "svm = LinearSVC().fit(X_train,y_train)\n",
    "print(classification_report( log.predict(X_test) , y_test))\n",
    "print(\"Logistic Regression:\",accuracy_score( log.predict(X_test) , y_test ),accuracy_score( log.predict(X_train) , y_train ))\n",
    "print(classification_report( d_tree.predict(X_test) , y_test))\n",
    "print(\"Decision Tree Accuracy:\",accuracy_score( d_tree.predict(X_test) , y_test ),accuracy_score( d_tree.predict(X_train) , y_train ))\n",
    "print(classification_report( clf.predict(X_test) , y_test))\n",
    "print(\"MultinomialNB Accuracy:\",accuracy_score( clf.predict(X_test) , y_test ),accuracy_score( clf.predict(X_train) , y_train ))\n",
    "print(classification_report( svm.predict(X_test) , y_test))\n",
    "print(\"SVM Accuracy:\",accuracy_score( svm.predict(X_test) , y_test ),accuracy_score( svm.predict(X_train) , y_train ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-53ed7ab3432b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0msvc_param_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msvc_param_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "####### Tuning Hyperparameters ####### \n",
    "\n",
    "#Grid Search For SVM\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [9.5,9]\n",
    "    gammas = [0.25,0.2]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas,'kernel' : ['rbf']}\n",
    "    grid_search = GridSearchCV(SVC(), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_score_,grid_search.best_params_\n",
    "\n",
    "svc_param_selection(X_train, y_train, 5)\n",
    "\n",
    "#Grid Search For Naive Bayes \n",
    "def svc_param_selection(X, y):\n",
    "    grid_params = {\n",
    "      'alpha': [0.35,0.3,0.25],\n",
    "      'fit_prior': [True, False]}\n",
    "    crossvalidation=KFold(n_splits=100,shuffle=True,random_state=1)\n",
    "    cl = GridSearchCV(clf, grid_params,cv=crossvalidation)\n",
    "    cl.fit(X, y)\n",
    "    return cl.best_params_,cl.best_score_\n",
    "##### Random Search for SVM\n",
    "\n",
    "\n",
    "rand_list = {\"C\": stats.uniform(2, 10),\n",
    "             \"gamma\": stats.uniform(0.01, 1),\n",
    "            \"kernel\":['rbf','linear','sigmoid']}\n",
    "              \n",
    "rand_search = RandomizedSearchCV(svm, param_distributions = rand_list, n_iter = 20, n_jobs = 4, cv = 3, random_state = 2) \n",
    "rand_search.fit(X_train, y_train) \n",
    "rand_search.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
