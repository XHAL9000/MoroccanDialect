{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import re\n",
    "import string\n",
    "import unicodedata as ud\n",
    "import codecs\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### GET DATA ##############\n",
    "#Call stop words \n",
    "def get_stop_words():\n",
    "    path = \"C:\\\\Users\\\\asus\\\\Desktop\\\\stop_words.txt\"\n",
    "    stop_words = []\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "        stop_words = myfile.readlines()\n",
    "    stop_words = [word.strip() for word in stop_words]\n",
    "    return stop_words\n",
    "#Call pos emojis \n",
    "def get_emojis():\n",
    "    with codecs.open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\emoji.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "         positive_emoji=myfile.read()\n",
    "    positive_emoji=positive_emoji.split(\"\\r\\n\")\n",
    "    positive_emoji=positive_emoji[1:len(positive_emoji)-1] \n",
    "    with codecs.open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\neg_emoji.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfie:\n",
    "        neg_emoji=myfie.read()\n",
    "    neg_emoji=neg_emoji.split(\"\\r\\n\")\n",
    "    neg_emoji=neg_emoji[1:len(neg_emoji)-1]\n",
    "    return positive_emoji,neg_emoji\n",
    "    \n",
    "\n",
    "#get data , Text vs label\n",
    "def get_data():\n",
    "    path=\"C:\\\\Users\\\\asus\\\\Desktop\\\\data.txt\"\n",
    "    with codecs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "        text = myfile.read()\n",
    "    sentences=text.split(\"@\")\n",
    "    sentences=[x.split(\"\\r\\n\") for x in sentences]\n",
    "    sentences=sentences[1:len(sentences)-1]\n",
    "    df=pd.DataFrame(sentences)\n",
    "    df.columns = ['sentiment','sentence']\n",
    "    return df\n",
    "\n",
    "############## PREPROCESSING DATA ################\n",
    "#Remove stop words \n",
    "def remove_stp_words(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    stop_words = get_stop_words()\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            text_words.append(word)\n",
    "    return ' '.join(text_words)\n",
    "#Emoticons_Regex \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "#Tokenizing_the_text :\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "#Normalize_the_text:\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "#Remove-repeating_char :\n",
    "def remove_repeating_char(text):\n",
    "     #return re.sub(r'(.)\\1+', r'\\1', text)     # keep only 1 repeat\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)     # keep 2 repeat\n",
    "#Detecting_Emojis_REGEX\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "#Transform_emojis-to_sentiment \n",
    "def emoji_to_text(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    positive_emoji,neg_emoji=get_emojis()\n",
    "    for word in words:\n",
    "        if emoji_pattern.search(word):\n",
    "            if word in positive_emoji :\n",
    "                word='ايجابي'\n",
    "            if word in neg_emoji :\n",
    "                word='سلبي'\n",
    "        text_words.append(word)\n",
    "    return ' '.join(text_words)\n",
    "#REMOVING_Punctuation\n",
    "def remove_punc(text):\n",
    "    return ''.join(c for c in text if not ud.category(c).startswith('P'))\n",
    "\n",
    "#Stemmer_LIGHT :\n",
    "ArListem = ArabicLightStemmer()\n",
    "def stemmer_light(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    for c in words:\n",
    "        stem = ArListem.light_stem(c)\n",
    "        text_words.append(stem)\n",
    "    return ' '.join(text_words)\n",
    "#Stemmer_root:\n",
    "def stemmer_root(text):\n",
    "    text_words = []\n",
    "    words = text.split(\" \")\n",
    "    for c in words:\n",
    "        stem = ArListem.light_stem(c)\n",
    "        text_words.append(stem)\n",
    "    return ' '.join(text_words)\n",
    "def correct(text):\n",
    "    #T=TextCorrection()\n",
    "    words = text.split(\" \")\n",
    "    text_words = []\n",
    "    for word in words :\n",
    "        text_words.append(T.correction(word,top=True))\n",
    "    return ' '.join(text_words)\n",
    "############ ARABIC TEXT PREPROCESSIG #########\n",
    "def data_clean(stp_words=False,stem=True,tok=True,emojis=False):\n",
    "    data = get_data()\n",
    "    sentences = data['sentence']\n",
    "    sentences = [araby.strip_tatweel(text) for text in sentences]\n",
    "    sentences = [remove_punc(text) for text in sentences]\n",
    "    if emojis==False :\n",
    "        sentences = [emoji_to_text(text) for text in sentences] \n",
    "    sentences = [remove_repeating_char(text) for text in sentences]\n",
    "    sentences = [araby.strip_tashkeel(text) for text in sentences]\n",
    "    if stp_words==True :\n",
    "        sentences = [remove_stp_words(text) for text in sentences]\n",
    "    #sentences = [correct(text) for text in sentences]\n",
    "    if stem==True:\n",
    "        sentences = [stemmer_light(text) for text in sentences]\n",
    "    sentences = [normalize_arabic(text) for text in sentences]\n",
    "    sentences = [araby.normalize_hamza(text) for text in sentences]\n",
    "    if tok==True:\n",
    "        sentences = [tokenize(text) for text in sentences]\n",
    "    return sentences         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######## TF_IDF ############\n",
    "def get_tfidf():\n",
    "    data = data_clean(stp_words=True,stem=False,tok=False,emojis=False)\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (1,1))\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    return X\n",
    "########## Document-Term Matrix(DTM) ######\n",
    "def doc_term_frec():\n",
    "    data = data_clean(stp_words=True,stem=False,tok=False,emojis=False)\n",
    "    cv = CountVectorizer(ngram_range = (1,1))\n",
    "    X= cv.fit_transform(data)\n",
    "    return X\n",
    "\n",
    "plt.bar([\"Positive\",\"Negative\"],[sentiment[sentiment=='P'].count(),sentiment[sentiment=='N'].count()],color=['g','b']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1642, 9112) (1642,)\n",
      "(411, 9112) (411,)\n"
     ]
    }
   ],
   "source": [
    "data_set =get_data()\n",
    "sentiment = data_set['sentiment']\n",
    "sentimen =np.array( [1 if sen == 'P' else 0 for sen in sentiment])\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_term_frec(), sentimen, test_size=0.2, random_state=1)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82       192\n",
      "           1       0.87      0.77      0.82       219\n",
      "\n",
      "    accuracy                           0.82       411\n",
      "   macro avg       0.82      0.82      0.82       411\n",
      "weighted avg       0.82      0.82      0.82       411\n",
      "\n",
      "Logistic Regression: 0.8175182481751825 0.9987819732034104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.74      0.79       248\n",
      "           1       0.66      0.79      0.72       163\n",
      "\n",
      "    accuracy                           0.76       411\n",
      "   macro avg       0.75      0.76      0.75       411\n",
      "weighted avg       0.77      0.76      0.76       411\n",
      "\n",
      "Decision Tree Accuracy: 0.7591240875912408 0.9993909866017052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.87      0.79       182\n",
      "           1       0.88      0.74      0.80       229\n",
      "\n",
      "    accuracy                           0.80       411\n",
      "   macro avg       0.80      0.81      0.80       411\n",
      "weighted avg       0.81      0.80      0.80       411\n",
      "\n",
      "MultinomialNB Accuracy: 0.7980535279805353 0.9957369062119367\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.87      0.79       181\n",
      "           1       0.88      0.74      0.80       230\n",
      "\n",
      "    accuracy                           0.80       411\n",
      "   macro avg       0.80      0.80      0.80       411\n",
      "weighted avg       0.81      0.80      0.80       411\n",
      "\n",
      "SVM Accuracy: 0.7956204379562044 0.9987819732034104\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.1, fit_prior= False).fit(X_train, y_train)\n",
    "log = LogisticRegression( solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)                                                                    \n",
    "d_tree = tree.DecisionTreeClassifier().fit(X_train,y_train)\n",
    "svm = LinearSVC().fit(X_train,y_train)\n",
    "print(classification_report( log.predict(X_test) , y_test))\n",
    "print(\"Logistic Regression:\",accuracy_score( log.predict(X_test) , y_test ),accuracy_score( log.predict(X_train) , y_train ))\n",
    "print(classification_report( d_tree.predict(X_test) , y_test))\n",
    "print(\"Decision Tree Accuracy:\",accuracy_score( d_tree.predict(X_test) , y_test ),accuracy_score( d_tree.predict(X_train) , y_train ))\n",
    "print(classification_report( clf.predict(X_test) , y_test))\n",
    "print(\"MultinomialNB Accuracy:\",accuracy_score( clf.predict(X_test) , y_test ),accuracy_score( clf.predict(X_train) , y_train ))\n",
    "print(classification_report( svm.predict(X_test) , y_test))\n",
    "print(\"SVM Accuracy:\",accuracy_score( svm.predict(X_test) , y_test ),accuracy_score( svm.predict(X_train) , y_train ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.65      0.78       288\n",
      "           1       0.53      0.97      0.68       118\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       406\n",
      "   macro avg       0.75      0.81      0.73       406\n",
      "weighted avg       0.85      0.74      0.75       406\n",
      "\n",
      "Random Forest: 0.7413793103448276 0.7925925925925926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.61      0.76       309\n",
      "           1       0.45      0.99      0.62        97\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       406\n",
      "   macro avg       0.72      0.80      0.69       406\n",
      "weighted avg       0.86      0.70      0.73       406\n",
      "\n",
      "Adaboost: 0.7044334975369458 0.7327160493827161\n"
     ]
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0).fit(X_train,y_train)\n",
    "adaboost = AdaBoostClassifier(n_estimators=800,learning_rate=0.009,random_state=0).fit(X_train,y_train)\n",
    "print(classification_report( rand_forest.predict(X_test) , y_test))\n",
    "print(\"Random Forest:\",accuracy_score( rand_forest.predict(X_test) , y_test ),accuracy_score( rand_forest.predict(X_train) , y_train ))\n",
    "print(classification_report( adaboost.predict(X_test),y_test))\n",
    "print(\"Adaboost:\",accuracy_score( adaboost.predict(X_test) , y_test ),accuracy_score( adaboost.predict(X_train) , y_train ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
